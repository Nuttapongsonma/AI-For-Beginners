## ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏Ñ‡πâ‡∏î MLP ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏î‡πâ‡∏ß‡∏¢ Pure Python:

### 1. ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Ñ‡∏•‡∏≤‡∏™ (Constructor)

- `__init__(self, layer_sizes)`: ‡∏£‡∏±‡∏ö‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏±‡πâ‡∏ô
- ‡∏™‡∏£‡πâ‡∏≤‡∏á weights ‡πÅ‡∏•‡∏∞ biases ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏±‡πâ‡∏ô‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Ñ‡πà‡∏≤‡∏™‡∏∏‡πà‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö weights ‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏®‡∏π‡∏ô‡∏¢‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö biases

### 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô (Activation Functions)

- `sigmoid(self, x)`: ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô sigmoid ‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0-1
- `sigmoid_derivative(self, x)`: ‡∏≠‡∏ô‡∏∏‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô sigmoid ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£ backpropagation
    
    <aside>
    üí°
    
    > ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô Sigmoid ‡πÑ‡∏î‡πâ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ S (S-shaped curve) ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ sigmoid curve ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏Ñ‡∏∑‡∏≠ f(x) = 1/(1 + e^(-x))
    > 
    
    > ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏û‡∏¥‡πÄ‡∏®‡∏©‡∏Ç‡∏≠‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô sigmoid ‡∏Ñ‡∏∑‡∏≠:
    > 
    
    > ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤ input ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ä‡πà‡∏ß‡∏á‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏ï‡πà -‚àû ‡∏ñ‡∏∂‡∏á +‚àû ‡πÉ‡∏´‡πâ‡∏Å‡∏•‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0 ‡∏ñ‡∏∂‡∏á 1
    > 
    
    > ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡∏´‡∏≤‡∏≠‡∏ô‡∏∏‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÑ‡∏î‡πâ (differentiable) ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ backpropagation
    > 
    
    > ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0 ‡∏ñ‡∏∂‡∏á 1
    > 
    
    > ‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ Sigmoid ‡∏°‡∏≤‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡∏Å‡∏£‡∏µ‡∏Å ‡πÇ‡∏î‡∏¢‡πÅ‡∏¢‡∏Å‡πÄ‡∏õ‡πá‡∏ô 2 ‡∏™‡πà‡∏ß‡∏ô‡∏Ñ‡∏∑‡∏≠:
    > 
    > - "Sigm-" ‡∏°‡∏≤‡∏à‡∏≤‡∏Å "sigma" (œÉ) ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡∏ï‡∏±‡∏ß S
    > - "-oid" ‡∏°‡∏≤‡∏à‡∏≤‡∏Å "eidos" (Œµ·º∂Œ¥ŒøœÇ) ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á "‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á" ‡∏´‡∏£‡∏∑‡∏≠ "‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞"
    > ‡∏£‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏à‡∏∂‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á "‡∏°‡∏µ‡∏£‡∏π‡∏õ‡∏£‡πà‡∏≤‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏±‡∏ß S"
    </aside>
    
    <aside>
    **‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô**
    
    ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å Sigmoid ‡πÅ‡∏•‡πâ‡∏ß ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ï‡∏∏‡πâ‡∏ô (Activation Functions) ‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡∏≠‡∏µ‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏ï‡∏±‡∏ß:
    
    - **ReLU (Rectified Linear Unit)**: f(x) = max(0,x)
        - ‡∏á‡πà‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì
        - ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏õ‡∏±‡∏ç‡∏´‡∏≤ vanishing gradient
        - ‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡∏°‡∏≤‡∏Å‡πÉ‡∏ô‡πÇ‡∏Ñ‡∏£‡∏á‡∏Ç‡πà‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏™‡∏≤‡∏ó‡πÄ‡∏ó‡∏µ‡∏¢‡∏°‡∏™‡∏°‡∏±‡∏¢‡πÉ‡∏´‡∏°‡πà
    - **Tanh (Hyperbolic Tangent)**: f(x) = (e^x - e^-x)/(e^x + e^-x)
        - ‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ Sigmoid ‡πÅ‡∏ï‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á -1 ‡∏ñ‡∏∂‡∏á 1
        - ‡∏°‡∏±‡∏Å‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ Sigmoid ‡πÉ‡∏ô‡∏´‡∏•‡∏≤‡∏¢‡∏Å‡∏£‡∏ì‡∏µ
    - **Leaky ReLU**: f(x) = max(0.01x, x)
        - ‡πÄ‡∏õ‡πá‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏≠‡πà‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á ReLU
        - ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ "dying ReLU"
    - **Softmax**: ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö output layer ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô classification
        - ‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô
        - ‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏Ç‡∏≠‡∏á‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö 1
    </aside>
    

### 3. Forward Propagation

- `forward(self, x)`: ‡∏™‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢
- ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤ activations ‡πÅ‡∏•‡∏∞ z_values ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏±‡πâ‡∏ô
- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ dot product ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á input ‡∏Å‡∏±‡∏ö weights ‡πÅ‡∏•‡∏∞‡∏ö‡∏ß‡∏Å‡∏î‡πâ‡∏ß‡∏¢ biases

### 4. Backward Propagation

- `backward(self, x, y, learning_rate)`: ‡∏õ‡∏£‡∏±‡∏ö‡∏Ñ‡πà‡∏≤ weights ‡πÅ‡∏•‡∏∞ biases
- ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î (delta) ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡πà‡∏≤ weights ‡πÅ‡∏•‡∏∞ biases ‡∏ï‡∏≤‡∏°‡∏Ñ‡πà‡∏≤ learning rate
- ‡πÉ‡∏ä‡πâ‡∏™‡∏π‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient descent ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î

